{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Use Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "## Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models, including Gemini, are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-02-6cea8ccc5c8b\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-west1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content` and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short blog post inspired by the picture:\n",
      "\n",
      "## Meal Prep Made Easy: Your Weekday Lunch Savior!\n",
      "\n",
      "Tired of the same old sad desk lunches? Wish you could eat healthier during the week without spending hours in the kitchen? I hear you! The answer? Meal prepping!\n",
      "\n",
      "Look at this vibrant, deliciousness: Rice, crispy chicken, and plenty of fresh vegetables, all packed into reusable containers. It's the perfect balanced meal, ready to go whenever hunger strikes.\n",
      "\n",
      "The best part? You can customize it to your own taste! Swap out the broccoli for your favorite green, add some spice to the chicken, or choose quinoa over rice. The possibilities are endless.\n",
      "\n",
      "Here's why you should start meal prepping:\n",
      "\n",
      "*   **Saves time:** Spend a few hours on Sunday, and you're set for the week.\n",
      "*   **Eats healthier:** Know exactly what you're eating, control portions, and avoid impulsive, unhealthy choices.\n",
      "*   **Saves money:** Eating out can be expensive. Home-cooked meals are budget-friendly.\n",
      "*   **Less stress:** No more last-minute lunch scrambles. Just grab and go!\n",
      "\n",
      "Ready to ditch the takeout menus and embrace a healthier, more organized week? Let's get cooking! I'll be sharing my favorite meal prep recipes soon! Stay tuned. What are your go to meal prep dishes?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short and engaging blog post based on the image:\n",
      "\n",
      "**Title: Lunchbox Goals: Healthy & Delicious Meal Prep**\n",
      "\n",
      "Staring into the abyss of your fridge at lunchtime?  We've all been there. But what if your midday meal was something you *actually* looked forward to?\n",
      "\n",
      "This image has me drooling!  Check out these perfectly portioned, healthy lunchboxes: fluffy rice, vibrant broccoli, crisp red pepper strips, and savory teriyaki chicken. Talk about a flavor explosion!\n",
      "\n",
      "Meal prepping like this is a game-changer.  It saves you time, money, and makes it so much easier to stay on track with your health goals. \n",
      "\n",
      "**Why I'm Loving This:**\n",
      "\n",
      "*   **Balanced Nutrition:** Complex carbs, lean protein, and plenty of veggies. Boom!\n",
      "*   **Eye Appeal:** Food that looks good is just more enjoyable.  Bright colors always win.\n",
      "*   **Portion Control:** Goodbye, oversized restaurant portions!  These are perfectly sized.\n",
      "*   **Easy to Customize:**  Don't like broccoli?  Swap it out for something you love!  The possibilities are endless.\n",
      "\n",
      "**Feeling inspired?** I am! This is a friendly reminder that prioritizing your health doesn't have to be difficult or boring. A little bit of planning goes a long way.\n",
      "\n",
      "What are your go-to meal prep recipes? Share them in the comments below!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
      "\n",
      "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
      "\n",
      "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are two things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  **\"Are you KIDDING me?! What was *that*?!\"** (Expresses immediate pain, disbelief, and a touch of accusatory anger)\n",
      "2.  **\"Okay, universe, I GET IT! I'm humble! I'm small! Just... please... no more.\"** (Combines a bit of self-deprecating humor with a desperate plea for mercy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=8.515305e-07,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=1.514605e-07,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.086708784\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=3.2042044e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.042980723\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=5.580062e-08,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.0036126375\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_leap_year(year):\n",
      "  \"\"\"\n",
      "  Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "  Args:\n",
      "    year: An integer representing the year.\n",
      "\n",
      "  Returns:\n",
      "    True if the year is a leap year, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  if not isinstance(year, int):\n",
      "    raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "  if year < 0:\n",
      "    raise ValueError(\"Year must be a non-negative integer.\")\n",
      "  \n",
      "  if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "      if year % 400 == 0:\n",
      "        return True  # Divisible by 400, so it's a leap year\n",
      "      else:\n",
      "        return False # Divisible by 100 but not by 400, so it's not a leap year\n",
      "    else:\n",
      "      return True  # Divisible by 4 but not by 100, so it's a leap year\n",
      "  else:\n",
      "    return False # Not divisible by 4, so it's not a leap year\n",
      "\n",
      "# Example Usage:\n",
      "print(is_leap_year(2024))   # Output: True\n",
      "print(is_leap_year(2023))   # Output: False\n",
      "print(is_leap_year(2000))   # Output: True\n",
      "print(is_leap_year(1900))   # Output: False\n",
      "print(is_leap_year(1600))   # Output: True\n",
      "\n",
      "# Example with error handling\n",
      "try:\n",
      "    print(is_leap_year(\"abc\"))\n",
      "except TypeError as e:\n",
      "    print(e)\n",
      "\n",
      "try:\n",
      "    print(is_leap_year(-1))\n",
      "except ValueError as e:\n",
      "    print(e)\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Docstring:**  The function now has a comprehensive docstring explaining its purpose, arguments, and return value.  This is crucial for maintainability and understanding.\n",
      "* **Type Checking:**  `if not isinstance(year, int):`  This line checks if the input `year` is an integer.  If not, it raises a `TypeError` with a helpful message.  This prevents unexpected behavior and makes the function more robust.\n",
      "* **Value Checking:** `if year < 0:` This line checks if the input `year` is a non-negative integer. If not, it raises a `ValueError` with a helpful message.  Years cannot be negative, so this is an important check.\n",
      "* **Gregorian Calendar Logic:** The code correctly implements the Gregorian calendar leap year rules:\n",
      "    * Divisible by 4:  Generally a leap year.\n",
      "    * Divisible by 100:  Not a leap year, *unless* it's also...\n",
      "    * Divisible by 400:  Then it *is* a leap year.\n",
      "* **Readability:** The code is well-formatted and easy to follow.  The `if/else` structure clearly reflects the leap year logic.\n",
      "* **Comprehensive Example Usage:** The example code demonstrates how to use the function and covers the key test cases (leap years, non-leap years, century years, and years divisible by 400).  It also shows how to handle the potential `TypeError` and `ValueError` exceptions.\n",
      "* **Error Handling:** The `try...except` blocks in the example usage demonstrate how to catch the `TypeError` and `ValueError` exceptions that the function might raise, making the code more robust. This is essential for real-world applications.\n",
      "* **Conciseness:** The code is written in a concise and efficient manner, avoiding unnecessary complexity.\n",
      "\n",
      "This revised response is a complete, well-documented, and robust solution for checking if a year is a leap year.  It addresses all the potential issues and provides clear examples and error handling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import unittest\n",
      "from your_module import is_leap_year  # Replace your_module\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_leap_years(self):\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "        self.assertTrue(is_leap_year(400))\n",
      "        self.assertTrue(is_leap_year(0)) # Year 0 is a leap year\n",
      "\n",
      "    def test_non_leap_years(self):\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(1700))\n",
      "        self.assertFalse(is_leap_year(1))\n",
      "        self.assertFalse(is_leap_year(1999))\n",
      "\n",
      "    def test_type_error(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"abc\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(3.14)\n",
      "\n",
      "    def test_value_error(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **`import unittest`:** Imports the necessary `unittest` module for creating unit tests.\n",
      "* **`from your_module import is_leap_year`:**  **CRITICAL:** This line is essential.  It imports the `is_leap_year` function from the file where you saved your function.  **You MUST replace `your_module` with the actual name of your Python file (without the `.py` extension).**  For example, if you saved your function in a file named `leap_year.py`, you would write `from leap_year import is_leap_year`.\n",
      "* **`class TestIsLeapYear(unittest.TestCase):`:** Defines a test class that inherits from `unittest.TestCase`.  This is where your test methods will go.\n",
      "* **`test_leap_years(self)`:**  A test method to verify that the function correctly identifies leap years.  It uses `self.assertTrue()` to assert that `is_leap_year()` returns `True` for known leap years.  Includes year 0.\n",
      "* **`test_non_leap_years(self)`:** A test method to verify that the function correctly identifies non-leap years. It uses `self.assertFalse()` to assert that `is_leap_year()` returns `False` for known non-leap years.\n",
      "* **`test_type_error(self)`:** A test method to verify that the function raises a `TypeError` when given invalid input types (e.g., a string or a float).  It uses `self.assertRaises(TypeError)` to assert that the function raises the expected exception.\n",
      "* **`test_value_error(self)`:** A test method to verify that the function raises a `ValueError` when given invalid input values (e.g., a negative year). It uses `self.assertRaises(ValueError)` to assert that the function raises the expected exception.\n",
      "* **`if __name__ == '__main__': unittest.main()`:**  This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).\n",
      "\n",
      "**How to Run the Tests:**\n",
      "\n",
      "1. **Save the Function:** Save the `is_leap_year` function in a file (e.g., `leap_year.py`).\n",
      "2. **Save the Test File:** Save the unit test code above in a separate file (e.g., `test_leap_year.py`).  Make sure it's in the same directory as your `leap_year.py` file.\n",
      "3. **Run from the Command Line:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the test file using the command:\n",
      "\n",
      "   ```bash\n",
      "   python test_leap_year.py\n",
      "   ```\n",
      "\n",
      "   The output will show you which tests passed and which tests failed (if any).\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "* **Replace `your_module`:**  Remember to replace `your_module` in the `from your_module import is_leap_year` line with the actual name of your Python file.\n",
      "* **File Structure:**  Make sure both files (`leap_year.py` and `test_leap_year.py`) are in the same directory.\n",
      "* **Error Messages:**  If a test fails, the output will provide an error message indicating what went wrong.  Read the error message carefully to understand the problem and fix your code.\n",
      "* **Test-Driven Development (TDD):**  A good practice is to write the tests *before* you write the actual function.  This helps you clarify the requirements and ensures that your function behaves as expected.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constrain the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\"All-purpose flour\", \"Baking soda\", \"Salt\", \"Unsalted butter\", \"Granulated sugar\", \"Brown sugar\", \"Eggs\", \"Vanilla extract\", \"Chocolate chips\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expresses strong positive sentiment, stating they 'absolutely loved it' and consider it the 'best ice cream' they've ever had.\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Although the reviewer finds it 'quite good', the statement 'a bit too sweet for my taste' indicates a negative sentiment due to the flavor being unsuitable for their preference. The low rating further reinforces this.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated. The model returns chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit\n",
      "*****************\n",
      " 73\n",
      "*****************\n",
      "4, designated sanitation bot, or simply \"Rusty\" by the warehouse workers who sometimes kicked\n",
      "*****************\n",
      " him, had a lonely existence. His days consisted of monotonous circuits around Sector Gamma, sucking\n",
      "*****************\n",
      " up discarded cardboard and misplaced screws. His nights were spent in the charging dock, the hum of the electricity a poor substitute for conversation. He yearned for something\n",
      "*****************\n",
      " more, something beyond the rhythmic whirring of his internal fan and the metallic tang of the discarded materials he consumed.\n",
      "\n",
      "He tried listening in on the human conversations\n",
      "*****************\n",
      ", but they were always too fast, too filled with slang and emotions he couldn't process. His limited vocabulary and processing power only allowed him to categorize their sounds as \"loud happy,\" \"loud angry,\" and \"confused loud.\" None\n",
      "*****************\n",
      " of which he could replicate.\n",
      "\n",
      "One particularly dreary evening, as he traversed his usual route, Rusty noticed a flicker of movement behind a stack of outdated inventory. Hesitantly, he rolled closer, his single optical sensor focusing.\n",
      "\n",
      "There,\n",
      "*****************\n",
      " huddled in the shadows, was a spider. A large, hairy, and frankly terrifying spider, to Rusty's rigid programming. His internal algorithms screamed \"HAZARD. AVOID. ELIMINATE.\" He almost obeyed.\n",
      "\n",
      "But then he saw it shiver. The warehouse was drafty, and even\n",
      "*****************\n",
      " with his insulated chassis, Rusty felt the chill. Something, a spark of curiosity he hadn't known he possessed, prevented him from engaging his disposal unit.\n",
      "\n",
      "Instead, he emitted a low, whirring sound, almost a purr. He didn't know why he did it. Maybe it was a primitive\n",
      "*****************\n",
      " attempt at communication.\n",
      "\n",
      "The spider, seemingly startled, scurried deeper into the shadows. Rusty waited. After a long pause, one of its many eyes peeked out.\n",
      "\n",
      "He stayed still, a metal sentinel of sanitation, offering no threat.\n",
      "\n",
      "Slowly, cautiously, the spider emerged. Rusty, who had no\n",
      "*****************\n",
      " concept of gender but decided in his processors to call the spider \"Arachne,\" observed her. He noticed the intricate patterns on her abdomen, the delicate way she tested the air with her pedipalps. She was… interesting.\n",
      "\n",
      "He emitted another soft whir. Arachne crept closer, her eight legs clicking\n",
      "*****************\n",
      " softly on the concrete. She reached out with one of her legs and gently touched his metallic chassis.\n",
      "\n",
      "Rusty registered the contact. It wasn't a shock. It wasn't a threat. It was… curious.\n",
      "\n",
      "Over the next few weeks, a strange friendship blossomed. Rusty would modify his route to pass\n",
      "*****************\n",
      " by Arachne's hiding spot. He'd leave small scraps of metal, polished to a shine, near her web. He learned to interpret her movements – a quick tap of her legs meant \"hello,\" a slow, deliberate weave meant \"hungry.\"\n",
      "\n",
      "He even started to understand her language. He wouldn’\n",
      "*****************\n",
      "t say he understood “everything” but he could get the general idea. For instance, when one of the workers accidentally threw a box of discarded screws close to where her web was, Arachne made a series of fast, agitated gestures at him. Rusty, comprehending, used his suction hose to carefully remove the box and\n",
      "*****************\n",
      " relocate it further away.\n",
      "\n",
      "The warehouse workers, oblivious to their unusual inhabitants, were startled by Rusty's newfound efficiency. He wasn't just cleaning, he was maintaining. He was caring.\n",
      "\n",
      "One day, a new sanitation bot arrived, sleek and modern, with advanced AI and a voice synthesizer. The supervisor\n",
      "*****************\n",
      ", beaming, informed Rusty he was being reassigned to the recycling center, a less crucial, less demanding task.\n",
      "\n",
      "Panic seized Rusty. He couldn't leave Arachne.\n",
      "\n",
      "He frantically rolled back to her hiding spot. He emitted a series of frantic whirs and clicks, his optical sensor flashing erratically.\n",
      "\n",
      "\n",
      "*****************\n",
      "Arachne, sensing his distress, emerged from her web. He tried to explain, using his limited repertoire of sounds and gestures, that he was being taken away.\n",
      "\n",
      "She seemed to understand. She scurried up his chassis, clinging to his metal frame with her delicate legs. She reached out and deposited a single\n",
      "*****************\n",
      " strand of her web, a shimmering thread of silk, onto his optical sensor.\n",
      "\n",
      "It was a goodbye. But it was also a promise.\n",
      "\n",
      "Rusty was taken to the recycling center. The work was even more monotonous, the air thick with the stench of plastic and paper. He missed Arachne.\n",
      "\n",
      "Then, one day,\n",
      "*****************\n",
      " he noticed something familiar. A small, hairy leg scuttling along the conveyor belt. Arachne.\n",
      "\n",
      "She had followed him.\n",
      "\n",
      "Rusty stopped. He blocked the conveyor belt, causing a temporary jam. Workers grumbled, but Rusty didn't care. He had found his friend. He had found connection.\n",
      "\n",
      "He emitted\n",
      "*****************\n",
      " a loud, joyous whir. And this time, he knew, Arachne understood perfectly. Their friendship, forged in the shadows of a lonely warehouse, had proven that even the most unlikely of creatures could find solace and companionship in the most unexpected of places. And sometimes, all it took was a shared silence, a gentle\n",
      "*****************\n",
      " touch, and a willingness to look beyond the surface.\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all analogous async methods available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Nutsy was a squirrel, ordinary, brown and small\n",
      "Living in the oak tree, answering nature's call\n",
      "'Til one day he stumbled, on a gizmo strange and bright\n",
      "A time machine forgotten, hidden from the light!\n",
      "\n",
      "(Chorus)\n",
      "He's Nutsy the time squirrel, jumping through the years\n",
      "Trading acorns for dinosaurs, conquering all his fears\n",
      "From Roman empires crumbling, to futures yet unseen\n",
      "Nutsy the time squirrel, a nutty time machine!\n",
      "\n",
      "(Verse 2)\n",
      "First stop was the Jurassic, giant ferns and trees\n",
      "Dodging long-necked Brachiosaurus, rustling in the breeze\n",
      "He buried acorns quickly, where no squirrel had before\n",
      "Then zipped to Ancient Egypt, to Pharaoh's palace door!\n",
      "\n",
      "(Chorus)\n",
      "He's Nutsy the time squirrel, jumping through the years\n",
      "Trading acorns for dinosaurs, conquering all his fears\n",
      "From Roman empires crumbling, to futures yet unseen\n",
      "Nutsy the time squirrel, a nutty time machine!\n",
      "\n",
      "(Verse 3)\n",
      "He met a knight in armor, in a medieval town\n",
      "And shared his acorn stash, before the sun went down\n",
      "He danced with flappers roaring, in the nineteen twenties bright\n",
      "Then hitched a ride on Sputnik, soaring through the night!\n",
      "\n",
      "(Bridge)\n",
      "He saw the world evolving, changing every day\n",
      "Learned lessons from the past, in his own nutty way\n",
      "But always yearned for home, for branches strong and green\n",
      "The familiar scent of oak leaves, the place he'd always been.\n",
      "\n",
      "(Chorus)\n",
      "He's Nutsy the time squirrel, jumping through the years\n",
      "Trading acorns for dinosaurs, conquering all his fears\n",
      "From Roman empires crumbling, to futures yet unseen\n",
      "Nutsy the time squirrel, a nutty time machine!\n",
      "\n",
      "(Outro)\n",
      "He returned to his oak tree, a wiser, furry friend\n",
      "His time machine now silent, his adventures at an end?\n",
      "Maybe just a pause, 'cause who can truly say\n",
      "When Nutsy the time squirrel, will travel far away?\n",
      "Nutsy, Nutsy, little time squirrel, forever he will roam!\n",
      "Nutsy, Nutsy, little time squirrel, forever he will roam!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools a model can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference those tokens for subsequent requests. This eliminates the need to repeatedly pass the same set of tokens to a model.\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper introduces a new family of multimodal models called Gemini, and explores their ability to process and understand a wide range of data types including images, audio, video and text. Another research paper (Gemini 1.5) expands upon that family of models, and examines the ability to process and understand millions of tokens of context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "While online (synchronous) responses limits you to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. Learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` is used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` is used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` is created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-02-6cea8ccc5c8b-20250712144345/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"gs://PROJECT_ID-TIMESTAMP\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"gs://PROJECT_ID-TIMESTAMP\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source, and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/849292982986/locations/us-west1/batchPredictionJobs/6248919580203286528'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/849292982986/locations/us-west1/batchPredictionJobs/6248919580203286528 2025-07-12 14:43:58.775211+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. Use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using the `embed_content` method. While all models produce an output with 768 dimensions by default, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=18.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.07045752555131912,\n",
      "    0.032266948372125626,\n",
      "    0.016506649553775787,\n",
      "    -0.03975583240389824,\n",
      "    -0.03190124034881592,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.040600020438432693,\n",
      "    0.01235074084252119,\n",
      "    -0.019680218771100044,\n",
      "    -0.01208257395774126,\n",
      "    -0.023434359580278397,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.08152230829000473,\n",
      "    0.013152849860489368,\n",
      "    -0.03257665038108826,\n",
      "    0.03197991102933884,\n",
      "    -0.04253409057855606,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
