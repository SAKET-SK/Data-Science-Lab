{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e2cdef-c152-4a71-847b-c0e55cfc015e",
   "metadata": {},
   "source": [
    "### Production Ready RAG : A Conceptual deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365160e4-ec2b-45e5-a121-0809f201609d",
   "metadata": {},
   "source": [
    "What we are going to do:\n",
    "- Compression saves tokens ‚Üí You'll see character counts drop\n",
    "- Routing saves money ‚Üí Simple queries use free model\n",
    "- Caching speeds things up ‚Üí Second query is instant\n",
    "- Metrics make it visible ‚Üí You can see everything happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f15f8d3-d97b-43da-8756-5d6dff969272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è Add GROQ_API_KEY to .env file!\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f1200-08bd-4a06-8722-5ad2c330a663",
   "metadata": {},
   "source": [
    "### Loading the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbee8be-161e-4df5-8363-d9d9b739033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 15 pages\n",
      "‚úÖ Created 37 chunks\n"
     ]
    }
   ],
   "source": [
    "# Load PDFs\n",
    "docs = []\n",
    "for file in os.listdir(\"./policies\"):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(f\"./policies/{file}\")\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} pages\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20977b-012b-4ada-b97b-38b6a164e0f5",
   "metadata": {},
   "source": [
    "### Creating a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d02322-2a21-48a8-b77f-3ae12b3832b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saket.khopkar\\AppData\\Local\\Temp\\ipykernel_17904\\3636965791.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store ready!\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_prod\")\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"mmr\",  # More diverse results\n",
    "    search_kwargs={\"k\": 1, \"fetch_k\": 10}  \n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b905904-3488-4a49-b151-cd0c5da64a73",
   "metadata": {},
   "source": [
    "### The Models to be set up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ec01ae-7891-4585-bd1c-624ceb80fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models ready:\n",
      "  üü¢ Simple (free): llama-3.1-8b-instant\n",
      "  üî¥ Complex (fast): llama-3.3-70b-versatile\n",
      "‚úÖ Compression enabled!\n",
      "\n",
      "üìä Compression test:\n",
      "  Before: 939 chars\n",
      "  After: 741 chars\n",
      "  Saved: 21%\n"
     ]
    }
   ],
   "source": [
    "# TWO MODELS: Simple (free) and Complex (paid)\n",
    "SIMPLE_MODEL = ChatGroq(model=\"llama-3.1-8b-instant\", api_key=groq_api_key, temperature=0)\n",
    "COMPLEX_MODEL = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=groq_api_key, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Models ready:\")\n",
    "print(\"  üü¢ Simple (free): llama-3.1-8b-instant\")\n",
    "print(\"  üî¥ Complex (fast): llama-3.3-70b-versatile\")\n",
    "\n",
    "# COMPRESSION: Extract only relevant parts from retrieved chunks\n",
    "compressor = LLMChainExtractor.from_llm(SIMPLE_MODEL)\n",
    "compressed_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Compression enabled!\")\n",
    "\n",
    "# Quick test\n",
    "test_docs_normal = retriever.get_relevant_documents(\"What is casual leave?\")\n",
    "test_docs_compressed = compressed_retriever.get_relevant_documents(\"What is casual leave?\")\n",
    "chars_normal = sum([len(d.page_content) for d in test_docs_normal])\n",
    "chars_compressed = sum([len(d.page_content) for d in test_docs_compressed])\n",
    "\n",
    "print(f\"\\nüìä Compression test:\")\n",
    "print(f\"  Before: {chars_normal} chars\")\n",
    "print(f\"  After: {chars_compressed} chars\")\n",
    "print(f\"  Saved: {((chars_normal - chars_compressed) / chars_normal * 100):.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbd485-ccb4-448b-b56e-c16170cf949c",
   "metadata": {},
   "source": [
    "### Model Routing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "184792fd-8cf6-4fbe-a10b-e20c54448340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing routing:\n",
      "\n",
      "Query: 'What is casual leave?'\n",
      "  üü¢ Using SIMPLE model\n",
      "\n",
      "Query: 'Compare casual leave vs earned leave'\n",
      "  üî¥ Using COMPLEX model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002504B12CB60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002504B12C680>, model_name='llama-3.3-70b-versatile', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def route_to_model(query: str):\n",
    "    \"\"\"\n",
    "    Route query to appropriate model based on complexity\n",
    "    \n",
    "    COMPLEX = comparisons, analysis, recommendations\n",
    "    SIMPLE = definitions, single facts\n",
    "    \"\"\"\n",
    "    complex_keywords = [\"compare\", \"difference\", \"recommend\", \"analyze\", \n",
    "                        \"evaluate\", \"best\", \"versus\", \"pros and cons\"]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Check if any complex keyword in query\n",
    "    for keyword in complex_keywords:\n",
    "        if keyword in query_lower:\n",
    "            print(\"  üî¥ Using COMPLEX model\")\n",
    "            return COMPLEX_MODEL\n",
    "    \n",
    "    print(\"  üü¢ Using SIMPLE model\")\n",
    "    return SIMPLE_MODEL\n",
    "\n",
    "# Test it\n",
    "print(\"üß™ Testing routing:\\n\")\n",
    "print(\"Query: 'What is casual leave?'\")\n",
    "route_to_model(\"What is casual leave?\")\n",
    "\n",
    "print(\"\\nQuery: 'Compare casual leave vs earned leave'\")\n",
    "route_to_model(\"Compare casual leave vs earned leave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56a4ec-7135-482c-8712-c08005a4b648",
   "metadata": {},
   "source": [
    "### Simple Cache + Metrics Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bebb497-d5e0-404d-a6b0-7aaea9bcbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Production RAG initialized!\n",
      "   - Cache ready\n",
      "   - Metrics ready\n",
      "   - Routing ready\n",
      "   - Compression ready\n"
     ]
    }
   ],
   "source": [
    "class ProductionRAG:\n",
    "    \"\"\"All-in-one: Cache + Metrics + Routing + Compression\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}  # Store answers\n",
    "        self.metrics = []  # Store query stats\n",
    "        \n",
    "    def _cache_key(self, query):\n",
    "        \"\"\"Create unique key for query\"\"\"\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def ask(self, query: str):\n",
    "        \"\"\"Main function: handles everything!\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. CHECK CACHE\n",
    "        cache_key = self._cache_key(query)\n",
    "        if cache_key in self.cache:\n",
    "            print(\"  ‚úÖ Cache HIT (instant!)\")\n",
    "            answer = self.cache[cache_key]\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Log metric\n",
    "            self.metrics.append({\n",
    "                \"query\": query[:50],\n",
    "                \"latency\": round(latency, 3),\n",
    "                \"source\": \"cache\"\n",
    "            })\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        print(\"  ‚ùå Cache MISS (calling LLM...)\")\n",
    "        \n",
    "        # 2. ROUTE TO MODEL\n",
    "        model = route_to_model(query)\n",
    "        model_name = \"simple\" if model == SIMPLE_MODEL else \"complex\"\n",
    "        \n",
    "        # 3. CREATE QA CHAIN (with compression!)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=model,\n",
    "            retriever=compressed_retriever,  # Using compression!\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        # 4. GET ANSWER\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        answer = result[\"result\"]\n",
    "        sources = [d.metadata.get(\"source\") for d in result[\"source_documents\"]]\n",
    "        \n",
    "        # 5. CACHE IT\n",
    "        self.cache[cache_key] = answer\n",
    "        \n",
    "        # 6. LOG METRICS\n",
    "        latency = time.time() - start_time\n",
    "        self.metrics.append({\n",
    "            \"query\": query[:50],\n",
    "            \"latency\": round(latency, 2),\n",
    "            \"source\": model_name,\n",
    "            \"docs\": sources\n",
    "        })\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def show_stats(self):\n",
    "        \"\"\"Display performance stats\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No queries yet!\")\n",
    "            return\n",
    "        \n",
    "        total = len(self.metrics)\n",
    "        cache_hits = sum(1 for m in self.metrics if m[\"source\"] == \"cache\")\n",
    "        avg_latency = sum(m[\"latency\"] for m in self.metrics) / total\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä PERFORMANCE STATS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total Queries: {total}\")\n",
    "        print(f\"Cache Hits: {cache_hits} ({cache_hits/total*100:.0f}%)\")\n",
    "        print(f\"Avg Latency: {avg_latency:.2f}s\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Initialize\n",
    "rag = ProductionRAG()\n",
    "\n",
    "print(\"‚úÖ Production RAG initialized!\")\n",
    "print(\"   - Cache ready\")\n",
    "print(\"   - Metrics ready\")\n",
    "print(\"   - Routing ready\")\n",
    "print(\"   - Compression ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77209ead-288a-4901-8f3a-a8e4842f3c9a",
   "metadata": {},
   "source": [
    "### Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e150d381-0d01-498e-86f8-837e6d33133a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING ALL FEATURES\n",
      "\n",
      "============================================================\n",
      "\n",
      "First query (simple):\n",
      "Query: 'What is casual leave?'\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üü¢ Using SIMPLE model\n",
      "Answer: Casual leave is a type of leave that employees can take for personal or miscellaneous reasons, such ...\n",
      "\n",
      "Same query again (should be instant):\n",
      "Query: 'What is casual leave?'\n",
      "  ‚úÖ Cache HIT (instant!)\n",
      "Answer: Casual leave is a type of leave that employees can take for personal or miscellaneous reasons, such ...\n",
      "\n",
      "Complex query:\n",
      "Query: 'Compare casual leave and sick leave'\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üî¥ Using COMPLEX model\n",
      "Answer: Based on the given context, here's a comparison between casual leave and sick leave:\n",
      "\n",
      "1. **Minimum d...\n",
      "\n",
      "==================================================\n",
      "üìä PERFORMANCE STATS\n",
      "==================================================\n",
      "Total Queries: 3\n",
      "Cache Hits: 1 (33%)\n",
      "Avg Latency: 0.44s\n",
      "==================================================\n",
      "\n",
      "‚úÖ All features working!\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ TESTING ALL FEATURES\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: First query (no cache, simple model)\n",
    "print(\"\\nFirst query (simple):\")\n",
    "print(\"Query: 'What is casual leave?'\")\n",
    "answer1 = rag.ask(\"What is casual leave?\")\n",
    "print(f\"Answer: {answer1[:100]}...\")\n",
    "\n",
    "# Test 2: Same query (cache hit!)\n",
    "print(\"\\nSame query again (should be instant):\")\n",
    "print(\"Query: 'What is casual leave?'\")\n",
    "answer2 = rag.ask(\"What is casual leave?\")\n",
    "print(f\"Answer: {answer2[:100]}...\")\n",
    "\n",
    "# Test 3: Complex query (uses complex model)\n",
    "print(\"\\nComplex query:\")\n",
    "print(\"Query: 'Compare casual leave and sick leave'\")\n",
    "answer3 = rag.ask(\"Compare casual leave and sick leave\")\n",
    "print(f\"Answer: {answer3[:100]}...\")\n",
    "\n",
    "# Show stats\n",
    "rag.show_stats()\n",
    "\n",
    "print(\"‚úÖ All features working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db603b7-8751-40c1-97c6-9777594ccd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "Commands:\n",
      "  'stats' - Show performance stats\n",
      "  'exit' - Quit and see final summary\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What are POSH Policy Guidelines\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üü¢ Using SIMPLE model\n",
      "\n",
      "ü§ñ Assistant: Based on the provided context, the POSH Policy Guidelines are as follows:\n",
      "\n",
      "1. **Purpose**: To create and maintain a safe work environment, free from sexual harassment and discrimination for all employees.\n",
      "2. **Scope**: The policy applies to all employees of BBIL SYSTEMS, including those working in company premises or elsewhere in India or abroad, and extends to clients, vendors, and contractors.\n",
      "3. **Applicability**: The policy applies to all employees of BBIL SYSTEMS, including those on permanent, temporary, contracted, or retainer ship basis, part-time basis, etc.\n",
      "4. **Definition of Employee**: An employee of BBIL SYSTEMS includes anyone carrying out work on behalf of the company, whether directly or indirectly, or through a vendor organization.\n",
      "\n",
      "These guidelines are based on the \"The Sexual harassment of women at workplace (prevention, prohibition & redressal) Act, 2013\" and aim to establish a zero-tolerance attitude towards any kind of sexual harassment or discrimination.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the job of Internal Committee? How do they work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üü¢ Using SIMPLE model\n",
      "\n",
      "ü§ñ Assistant: According to the given context, the job of the Internal Committee is to deal with complaints of Sexual Harassment in a confidential and urgent manner. They are responsible for conducting an official internal enquiry.\n",
      "\n",
      "Here's a breakdown of their roles and responsibilities:\n",
      "\n",
      "1. **Composition**: The Internal Committee consists of:\n",
      "\t* 1 Presiding Officer (1 member)\n",
      "\t* 3 Internal Members\n",
      "\t* 1 External Member (an NGO or Legal expert)\n",
      "2. **Responsibilities**: Within 3 working days of receiving a complaint, the Internal Committee shall:\n",
      "\t* Commence an official internal enquiry\n",
      "\t* Handle the complaint with utmost confidentiality and urgency\n",
      "3. **Composition of the Committee**: The committee is composed of a mix of internal and external members to ensure a fair and impartial investigation.\n",
      "\n",
      "The Internal Committee's primary goal is to investigate complaints of Sexual Harassment, ensure a fair and confidential process, and take necessary actions to address the issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How many casual leaves I get during a year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üü¢ Using SIMPLE model\n",
      "\n",
      "ü§ñ Assistant: Unfortunately, the given context doesn't mention the number of casual leaves you get per year. It only mentions the minimum and maximum duration of casual leave (0.5 to 3 days) and that there are no carry-forwards.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Any idea on how my annual salary increses?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚ùå Cache MISS (calling LLM...)\n",
      "  üü¢ Using SIMPLE model\n",
      "\n",
      "ü§ñ Assistant: Based on the given context, it seems that your annual salary increase would be determined by two main factors:\n",
      "\n",
      "1. Increase in the cost of living: This suggests that your salary may increase to keep pace with inflation and the rising cost of living.\n",
      "2. The Company's economic situation: This implies that the company's financial health and performance may also impact your salary increase.\n",
      "\n",
      "However, without more specific information on how these factors are applied to determine salary increases, it's difficult to provide a detailed answer. \n",
      "\n",
      "In general, it seems that your salary increase would be influenced by a combination of external (cost of living) and internal (company's economic situation) factors.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä PERFORMANCE STATS\n",
      "==================================================\n",
      "Total Queries: 7\n",
      "Cache Hits: 1 (14%)\n",
      "Avg Latency: 0.56s\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üëã Goodbye!\n",
      "\n",
      "\n",
      "==================================================\n",
      "üìä PERFORMANCE STATS\n",
      "==================================================\n",
      "Total Queries: 7\n",
      "Cache Hits: 1 (14%)\n",
      "Avg Latency: 0.56s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nCommands:\")\n",
    "print(\"  'stats' - Show performance stats\")\n",
    "print(\"  'exit' - Quit and see final summary\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \").strip()\n",
    "    \n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"\\nüëã Goodbye!\\n\")\n",
    "        rag.show_stats()\n",
    "        break\n",
    "    \n",
    "    if query.lower() == \"stats\":\n",
    "        rag.show_stats()\n",
    "        continue\n",
    "    \n",
    "    if not query:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print()  # Blank line for routing message\n",
    "        answer = rag.ask(query)\n",
    "        print(f\"\\nü§ñ Assistant: {answer}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48dc2ff2-f8f4-4ff0-b856-489c51d38d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics saved to: rag_metrics_20251013_165043.json\n",
      "   Total queries: 7\n",
      "   Cache hit rate: 14%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'rag_metrics_20251013_165043.json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save all metrics to file for later analysis\n",
    "\"\"\"\n",
    "\n",
    "def save_metrics():\n",
    "    \"\"\"Save performance data to JSON\"\"\"\n",
    "    \n",
    "    if not rag.metrics:\n",
    "        print(\"‚ö†Ô∏è No metrics to save. Try asking some questions first!\")\n",
    "        return\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    total = len(rag.metrics)\n",
    "    cache_hits = sum(1 for m in rag.metrics if m[\"source\"] == \"cache\")\n",
    "    avg_latency = sum(m[\"latency\"] for m in rag.metrics) / total\n",
    "    \n",
    "    data = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"summary\": {\n",
    "            \"total_queries\": total,\n",
    "            \"cache_hits\": cache_hits,\n",
    "            \"cache_hit_rate\": f\"{cache_hits/total*100:.1f}%\",\n",
    "            \"avg_latency\": f\"{avg_latency:.2f}s\"\n",
    "        },\n",
    "        \"all_queries\": rag.metrics\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    filename = f\"rag_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Metrics saved to: {filename}\")\n",
    "    print(f\"   Total queries: {total}\")\n",
    "    print(f\"   Cache hit rate: {cache_hits/total*100:.0f}%\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Save metrics\n",
    "save_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e8e7c-8cf3-4aa3-a8f9-121e6425e3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
