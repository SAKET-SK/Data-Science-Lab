{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae90a29b-1eea-43ea-8570-f776df2e278b",
   "metadata": {},
   "source": [
    "### Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288c135-1065-4167-bc60-47d6406d3c02",
   "metadata": {},
   "source": [
    "<b>Goal:</b> Make your RAG pipeline interactive and memory-aware\n",
    "\n",
    "<b>Key Concepts:</b>\n",
    "- Conversational Retrieval\n",
    "- Chat Memory\n",
    "- Answer grounding\n",
    "- Source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70276cf3-9194-4550-8b21-0c7aec2dbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6093dc85-0ae0-4869-bc12-08f7ce300d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641404e-124e-47d5-911e-8636ecfd2dec",
   "metadata": {},
   "source": [
    "### Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b47629-b437-41eb-8627-81ed741ebe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 15 pages from 3 documents:\n",
      "- Leave-Policy.pdf\n",
      "- posh-policy.pdf\n",
      "- Salary_Policy.pdf\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PDFs\n",
    "pdf_dir = \"policies\"\n",
    "\n",
    "# Load all PDFs\n",
    "loaders = [PyPDFLoader(os.path.join(pdf_dir, file)) for file in os.listdir(pdf_dir) if file.endswith(\".pdf\")]\n",
    "\n",
    "# Load and combine documents\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} pages from {len(loaders)} documents:\")\n",
    "for file in os.listdir(pdf_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(\"-\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b59c07-48e3-4c2e-b37e-e70c26bd0688",
   "metadata": {},
   "source": [
    "### Splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75cb171-2067-4871-b0a4-bd6e277292f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split into 37 text chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split into {len(chunks)} text chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b52b-c23b-486a-b2ba-83f966098656",
   "metadata": {},
   "source": [
    "Each ‚Äúchunk‚Äù is like a mini paragraph the AI can understand and embed.\n",
    "This prevents token overflow and improves retrieval precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b7178-1b72-4b82-9151-a98e7a76d97c",
   "metadata": {},
   "source": [
    "### Create Vector Store (Chroma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104ed022-89f6-4abe-8213-5ec5da7dc203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saket.khopkar\\AppData\\Local\\Temp\\ipykernel_17436\\1880215437.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created and retriever ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Using a free open-source embedding model to avoid OpenAI key issues\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create or reuse a Chroma vector database\n",
    "vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=\"chroma_db\")\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "print(\"‚úÖ Vector store created and retriever ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c000d-b94a-4989-91ff-dd42fae7b04a",
   "metadata": {},
   "source": [
    "\"k\": 1 ensures we retrieve the most relevant chunk to avoid token overflow.<br>\n",
    "If you set k=3, it retrieves 3 most similar chunks, but that increases prompt size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e184a-b812-4b59-8480-f22bb9e61660",
   "metadata": {},
   "source": [
    "### Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dd1b97-a479-4b2d-ad20-3e62732d6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Groq LLM successfully.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\", \n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connected to Groq LLM successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc776d7b-9132-4ac7-a31c-7c57cad574e9",
   "metadata": {},
   "source": [
    "### Creating Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5b93a5-ee22-4e89-b419-416e91ca392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversation memory initialized.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "print(\"‚úÖ Conversation memory initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68860168-cea0-4d91-9f0e-3b7b6e44a2d0",
   "metadata": {},
   "source": [
    "This memory stores past user‚ÄìAI exchanges.\n",
    "\n",
    "Without it, the system would treat every question as a new, unrelated query.\n",
    "\n",
    "Think of it as the ‚Äúchat log‚Äù behind your assistant‚Äôs responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3c553-79c8-4a4b-8a23-b4080c88caf9",
   "metadata": {},
   "source": [
    "### Building the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0dc7ad-96f3-4cd2-8ea3-b3a41f752a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversational Retrieval Chain is ready.\n"
     ]
    }
   ],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Conversational Retrieval Chain is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36859ccc-ec63-4572-9576-bb63748830de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Conversational RAG Assistant Ready!\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is Casual Leave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßë‚Äçüíª You asked: What is Casual Leave\n",
      "\n",
      "ü§ñ Assistant: Casual Leave is a type of leave that an employee can take when they need a day off for personal or miscellaneous reasons, not related to illness or injury. It is a paid leave, but has certain limitations and rules. Casual Leave can be taken for a minimum of 0.5 days to a maximum of 3 days, and any remaining unused leave lapses at the end of the year.\n",
      "\n",
      "üìö Sources:\n",
      "- policies\\Leave-Policy.pdf\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the step to register complaints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßë‚Äçüíª You asked: What is the step to register complaints\n",
      "\n",
      "ü§ñ Assistant: You can register complaints through the following methods:\n",
      "\n",
      "1. Email: Submit your complaint to hrteam@BBIL Systems.com.\n",
      "2. In-person discussion: You can discuss your complaint with any member of the Internal Committee mentioned in the policy within 3 months of the occurrence of the act of Sexual Harassment.\n",
      "\n",
      "üìö Sources:\n",
      "- policies\\posh-policy.pdf\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the salary document about?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßë‚Äçüíª You asked: What is the salary document about?\n",
      "\n",
      "ü§ñ Assistant: The salary document appears to be a public version of a salary policy. It is likely a guide or a set of rules that outlines how salaries are structured, paid, and managed within an entity. The document is part of a larger document set, titled \"Talent Management and Culture\" with a code of GT-02 and a revision number of 03.\n",
      "\n",
      "üìö Sources:\n",
      "- policies\\Salary_Policy.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí¨ Conversational RAG Assistant Ready!\")\n",
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "chat_history = []  # optional, if you want to keep record manually\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \").strip()\n",
    "    if query.lower() in (\"exit\", \"quit\"):\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # ‚úÖ print user query explicitly (so it's visible in notebook output)\n",
    "    print(f\"\\nüßë‚Äçüíª You asked: {query}\\n\")\n",
    "\n",
    "    try:\n",
    "        # run retrieval chain\n",
    "        result = qa_chain.invoke({\"question\": query})\n",
    "        answer = result.get(\"answer\", \"(no answer found)\")\n",
    "\n",
    "        # save to chat history (optional)\n",
    "        chat_history.append({\"user\": query, \"assistant\": answer})\n",
    "\n",
    "        # ‚úÖ print assistant's answer\n",
    "        print(\"ü§ñ Assistant:\", answer)\n",
    "\n",
    "        # ‚úÖ print sources (if available)\n",
    "        if \"source_documents\" in result:\n",
    "            print(\"\\nüìö Sources:\")\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                print(f\"- {doc.metadata.get('source', 'Unknown file')}\")\n",
    "        print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d0fe6-a528-4449-9fc7-70a2d5b93b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
