{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8e942d-1925-4fd1-abe4-e1d6116e2949",
   "metadata": {},
   "source": [
    "### RAG → Expand KB with policies, FAQs, and past conversations; add chunking & vector retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f3af0f-ceb5-4b4c-bea0-69504800c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 16.0 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\saket.khopkar\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (4.55.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: groq<1,>=0.30.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain-groq) (0.31.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saket.khopkar\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once in the notebook (may take a minute)\n",
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-groq faiss-cpu sentence-transformers transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1131ec19-fd2a-44e3-9fe4-002ec40b3233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ key present: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "load_dotenv()  # loads GROQ_API_KEY from .env if present\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Please set GROQ_API_KEY in a .env file or environment variable before running this notebook.\")\n",
    "\n",
    "# LangChain + helpers\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Groq LLM wrapper for LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# For optional saving/inspection\n",
    "import pickle, csv\n",
    "\n",
    "print(\"GROQ key present:\", bool(GROQ_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288f5516-fbf6-4719-a7ca-3048e0054cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saket.khopkar\\AppData\\Local\\Temp\\ipykernel_416\\3741587406.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Embedder using sentence-transformers (free, local)\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# Chunking config\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)\n",
    "\n",
    "print(\"Embedding model:\", EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f096908a-add4-420b-ba55-ee529f830945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 docs from ./policies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "kb_documents: List[Document] = []\n",
    "\n",
    "policies_dir = \"./policies\"\n",
    "\n",
    "if os.path.isdir(policies_dir) and any(os.scandir(policies_dir)):\n",
    "    for entry in os.scandir(policies_dir):\n",
    "        if entry.is_file():\n",
    "            # Load .txt and .md files\n",
    "            if entry.name.lower().endswith((\".txt\", \".md\")):\n",
    "                loader = TextLoader(entry.path, encoding=\"utf8\")\n",
    "                docs = loader.load()\n",
    "                for d in docs:\n",
    "                    d.metadata = {**d.metadata, \"source\": entry.name, \"type\": \"policy\"}\n",
    "                kb_documents.extend(docs)\n",
    "\n",
    "            # Load .pdf files\n",
    "            elif entry.name.lower().endswith(\".pdf\"):\n",
    "                pdf_reader = PdfReader(entry.path)\n",
    "                full_text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        full_text += text + \"\\n\"\n",
    "                if full_text.strip():\n",
    "                    kb_documents.append(Document(\n",
    "                        page_content=full_text,\n",
    "                        metadata={\"source\": entry.name, \"type\": \"policy\"}\n",
    "                    ))\n",
    "\n",
    "    print(f\"✅ Loaded {len(kb_documents)} docs from {policies_dir}.\")\n",
    "\n",
    "else:\n",
    "    # Fallback sample KB\n",
    "    samples = [\n",
    "        {\"source\": \"refund_policy.txt\", \"type\": \"policy\",\n",
    "         \"text\": \"Refund Policy: Customers can request refunds within 30 days of purchase. Refunds are processed in 5-7 business days. Digital goods are non-refundable once downloaded.\"},\n",
    "        {\"source\": \"shipping_policy.txt\", \"type\": \"policy\",\n",
    "         \"text\": \"Shipping Policy: Orders ship within 2 business days. Standard shipping: 5-7 days. Express shipping: 2-3 days. Free shipping for orders above $50.\"},\n",
    "        {\"source\": \"faq.csv\", \"type\": \"faq\",\n",
    "         \"text\": \"FAQ: Q: How can I track my order? A: Use your tracking link or enter order id on /track. Q: How to contact support? A: Email support@example.com or use live chat 9am-6pm Mon-Fri.\"},\n",
    "        {\"source\": \"conv1.json\", \"type\": \"history\",\n",
    "         \"text\": \"Conversation: User: I ordered 5 days ago and haven't received it. Agent: Please provide order id. User: 1234. Agent: It shows shipped 5 days ago.\"},\n",
    "        {\"source\": \"conv2.json\", \"type\": \"history\",\n",
    "         \"text\": \"Conversation: User: I downloaded the eBook and request refund. Agent: Policy: digital goods non-refundable after download.\"}\n",
    "    ]\n",
    "    for s in samples:\n",
    "        kb_documents.append(Document(\n",
    "            page_content=s[\"text\"],\n",
    "            metadata={\"source\": s[\"source\"], \"type\": s[\"type\"]}\n",
    "        ))\n",
    "\n",
    "    print(f\"⚠️ Using fallback sample KB: {len(kb_documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07339025-83e8-4f58-abf2-9088f7472e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 source docs into 11 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split into smaller chunks for better retrieval\n",
    "# These settings work well for both PDF and text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,    # Max characters per chunk\n",
    "    chunk_overlap=100, # Overlap to keep context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(kb_documents)\n",
    "print(f\"Split {len(kb_documents)} source docs into {len(split_docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7355787-c8af-4b23-a758-77e8eb58768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever is ready — can fetch relevant chunks from policies/\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store documents in a FAISS vector DB\n",
    "vectorstore = FAISS.from_documents(kb_documents, embeddings)\n",
    "\n",
    "# Retriever for semantic search\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"✅ Retriever is ready — can fetch relevant chunks from policies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0ac6cd6-4b47-41d1-9128-f193ad3a8f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq LLM initialized with llama-3.1-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "# Initialize the Groq LLM\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name=\"llama-3.1-8b-instant\"  # Recommended replacement for mixtral\n",
    ")\n",
    "print(\"✅ Groq LLM initialized with llama-3.1-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff124d93-700c-4b31-8a86-d94929de2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RetrievalQA chain created — ready for Q&A\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Build RetrievalQA chain with source docs enabled\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"✅ RetrievalQA chain created — ready for Q&A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "781757af-7e14-40cd-8467-c0fb716b7cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  What is Casual Leave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "According to the provided context, Casual Leave (CL) is a type of leave that is granted to employees for unforeseen situations or personal matters that require their attention. \n",
      "\n",
      "Here are some key characteristics of Casual Leave:\n",
      "\n",
      "1. **Maximum 3 days in a month**: Employees can take a maximum of 3 Casual Leave days in a month.\n",
      "2. **Permission in advance**: Employees need to take permission in advance for Casual Leave, except in cases of unforeseen situations.\n",
      "3. **Minimum 0.5 to maximum 3 days**: Casual Leave can be taken for a minimum of 0.5 days to a maximum of 3 days.\n",
      "4. **Not encashable**: Casual Leave is not encashable, meaning it cannot be converted into cash.\n",
      "5. **No carry-forward**: Unused Casual Leave lapses automatically at the end of the year.\n",
      "6. **Not clubbable with other leaves**: Casual Leave cannot be clubbed with Earned/Privileged Leave or Sick Leave.\n",
      "7. **Pro-rata for new joiners and resigned employees**: Casual Leave for new joiners and resigned employees is calculated on a pro-rata basis.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  Whom should inform before taking leaves\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "According to the provided leave policy, employees need to apply for each leave and take approval except in cases where approval could not be taken in advance, usually for casual or sick leaves.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  Who approves the leaves?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "According to the leave policy, the grant of leave shall depend upon the policies of the workplace and is at the discretion of the manager/management. This means that the manager or higher management has the authority to approve or reject leave applications.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  As a male associate, can I take Maternity leave?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "No, you cannot take Maternity leave as per the provided context. Maternity Leave is covered by Maternity Benefit Act and is specifically mentioned to be applicable to female employees. It is a leave provided for female employees to take care of their pregnancy, childbirth, and related complications.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  I have to go voting in my village, can i get a leave?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "I don't know if you will be able to get a leave to vote in your village. The leave policy document doesn't specifically mention \"Leave for Voting\" as an official type of leave, but it does mention \"Leave for Voting\" as being at the organization's discretion, along with other unpaid or half-paid leaves like Study Leave and Bereavement Leave.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  I am a new joiner, what leaves can I take?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "As a new joiner, you are entitled to pro-rated leaves. This means that you will get a fraction of the total leaves based on the number of days you have worked.\n",
      "\n",
      "You can take the following leaves as a new joiner:\n",
      "\n",
      "1. Casual Leave (CL): You can take a maximum of 3 days of CL in a month, but since you are new, your CL will be pro-rated based on the number of days you have worked.\n",
      "2. Sick Leave (SL): You can take a maximum of 7 days of SL, and your SL will also be pro-rated based on the number of days you have worked.\n",
      "3. Earned Leave (EL) or Privilege Leave (PL): You will be entitled to a fraction of the EL or PL based on the number of working days you have completed. Since you are new, you will not have enough working days to accumulate a full EL or PL.\n",
      "\n",
      "It's worth noting that you should check with your HR or management to confirm the details of your leave entitlement as a new joiner.\n",
      "\n",
      "Sources:\n",
      "- Leave-Policy.pdf (policy)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit' to quit): \").strip()\n",
    "    if query.lower() in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "        print(\"\\nAnswer:\")\n",
    "        print(result[\"result\"])  # Safe — always a string\n",
    "\n",
    "        print(\"\\nSources:\")\n",
    "        for src_doc in result[\"source_documents\"]:\n",
    "            print(f\"- {src_doc.metadata.get('source')} ({src_doc.metadata.get('type')})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67218ae-ebe6-4c6c-9563-221fd0552c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
